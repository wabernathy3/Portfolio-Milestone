{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "### Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the absolute path of the lexicon file to this program\n",
    "# example call:\n",
    "# nancymacpath = \n",
    "#    \"/Users/njmccrac/AAAdocs/research/subjectivitylexicon/hltemnlp05clues/subjclueslen1-HLTEMNLP05.tff\"\n",
    "# SL = readSubjectivity(nancymacpath\n",
    "dirPath = '/Users/wa3/Syracuse/Term6/NLP/Final/FinalProjectData/EmailSpamCorpora/corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  This program shell reads email data for the spam classification problem.\n",
    "  The input to the program is the path to the Email directory \"corpus\" and a limit number.\n",
    "  The program reads the first limit number of ham emails and the first limit number of spam.\n",
    "  It creates an \"emaildocs\" variable with a list of emails consisting of a pair\n",
    "    with the list of tokenized words from the email and the label either spam or ham.\n",
    "  It prints a few example emails.\n",
    "  Your task is to generate features sets and train and test a classifier.\n",
    "\n",
    "  Usage:  python classifySPAM.py  <corpus directory path> <limit number>\n",
    "'''\n",
    "# open python and nltk packages needed for processing\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read spam and ham files, train and test a classifier \n",
    "def processspamham(dirPath,limitStr):\n",
    "  # convert the limit argument from a string to an int\n",
    "  limit = int(limitStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "processspamham(dirPath, 1500)\n",
    "# start lists for spam and ham email texts\n",
    "limit = 1500\n",
    "#allfiles = 3672\n",
    "hamtexts = []\n",
    "spamtexts = []\n",
    "os.chdir(dirPath)\n",
    "  # process all files in directory that end in .txt up to the limit\n",
    "  #    assuming that the emails are sufficiently randomized\n",
    "for file in os.listdir(\"./spam\"):\n",
    "    if (file.endswith(\".txt\")) and (len(spamtexts) < limit):\n",
    "      # open file for reading and read entire file into a string\n",
    "      f = open(\"./spam/\"+file, 'r', encoding=\"latin-1\")\n",
    "      spamtexts.append (f.read())\n",
    "      f.close()\n",
    "for file in os.listdir(\"./ham\"):\n",
    "    if (file.endswith(\".txt\")) and (len(hamtexts) < limit):\n",
    "      # open file for reading and read entire file into a string\n",
    "      f = open(\"./ham/\"+file, 'r', encoding=\"latin-1\")\n",
    "      hamtexts.append (f.read())\n",
    "      f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam files: 1500\n",
      "Number of ham files: 1500\n"
     ]
    }
   ],
   "source": [
    "  # print number emails read\n",
    "print (\"Number of spam files:\",len(spamtexts))\n",
    "print (\"Number of ham files:\",len(hamtexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of mixed spam and ham email documents as (list of words, label)\n",
    "emaildocs = []\n",
    "  # add all the spam\n",
    "for spam in spamtexts:\n",
    "    tokens = nltk.word_tokenize(spam)\n",
    "    emaildocs.append((tokens, 'spam'))\n",
    "  # add all the regular emails\n",
    "for ham in hamtexts:\n",
    "    tokens = nltk.word_tokenize(ham)\n",
    "    emaildocs.append((tokens, 'ham'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the list\n",
    "random.Random(123).shuffle(emaildocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject', ':', 'accrued', 'thru', 'nov', '.', '19', 'daren', ',', 'please', 'take', 'a', 'look', 'at', 'the', 'attached', 'spreadsheet', ',', 'we', 'have', 'over', '500', ',', '000', 'mmbtu', 'as', 'unaccounted', 'for', '.', '.', '.', 'o', \"'\", 'neal', '3', '-', '9686']\n",
      "ham\n"
     ]
    }
   ],
   "source": [
    "# print a few token lists\n",
    "for email in emaildocs[0]:\n",
    "    print (email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44574\n"
     ]
    }
   ],
   "source": [
    "# get all words from all emails and put into a frequency distribution\n",
    "# note lowercase, but no stemming or stopwords\n",
    "all_words_list = [word for (sent,cat) in emaildocs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Started with 1500 words, 2000 words = higher accuracy\n",
    "4000 is peak accuracy at 96.1, 5000 = no change, 6000 starts to bring down accuracy with 95.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1500 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(4000)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "test_features = [word for (word,count) in emaildocs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 50 Words in Email Corpus before Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '.', ',', '/', ':', 'the', 'to', 'and', 'of', 'a']\n"
     ]
    }
   ],
   "source": [
    "#word_items[0:50]\n",
    "print((word_features[0:10]))\n",
    "#print(test_features[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most common words in the corpus are actually symbols and punctuation marks.  It will be interesting to see how these symbols and punctuations affect predictions of spam vs ham.  I would think that these might actually help with prediction based on my personal experience with spam emails.  It seems that spam emails in my inbox typically contain random symbols.  This will be something worth experiementing with in the future.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features sets for a document, including keyword features and category feature\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Word Count Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features (keywords) of a document for a BOW/unigram baseline\n",
    "# each feature is 'contains(keyword)' and is true or false depending\n",
    "# on whether that keyword is in the document\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['{}'.format(word)] = word_features.count(word)\n",
    "    #word_items\n",
    "    #for word in word_features:\n",
    "        #features['{}'.format(word)] = word_features.count(word)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features sets for a document, including keyword features and category feature\n",
    "#featuresets = [(document_features(d, all_words_list), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train/test set & classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training using naive Baysian classifier, training set is roughly 80% of the data\n",
    "train_set, test_set = featuresets[600:], featuresets[:600]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 80/20 Split of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "2400\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(featuresets))\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9816666666666667"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the accuracy of the classifier\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross-validation ##\n",
    "# this function takes the number of folds, the feature sets\n",
    "# it iterates over the folds, using different sections for training and testing in turn\n",
    "#   it prints the accuracy for each fold and the average accuracy at the end\n",
    "def cross_validation_accuracy(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier, test_this_round)\n",
    "        print (i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "    # find mean accuracy over all rounds\n",
    "    print ('mean accuracy', sum(accuracy_list) / num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 600\n",
      "0 0.9816666666666667\n",
      "1 0.9533333333333334\n",
      "2 0.9533333333333334\n",
      "3 0.9366666666666666\n",
      "4 0.9716666666666667\n",
      "mean accuracy 0.9593333333333334\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    \tgoldlist.append(label)\n",
    "    \tpredictedlist.append(classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam']\n",
      "['ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam']\n",
      "     |   s     |\n",
      "     |   p   h |\n",
      "     |   a   a |\n",
      "     |   m   m |\n",
      "-----+---------+\n",
      "spam |<312>  1 |\n",
      " ham |  10<277>|\n",
      "-----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the first 10 examples\n",
    "print(goldlist[:15])\n",
    "print(predictedlist[:15])\n",
    "\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output:  prints precision, recall and F1 for each label\n",
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.965      0.996      0.981\n",
      "spam \t      0.997      0.969      0.983\n"
     ]
    }
   ],
   "source": [
    "# call the function with our data\n",
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               forwarded = True              ham : spam   =    197.3 : 1.0\n",
      "                     hou = True              ham : spam   =    186.3 : 1.0\n",
      "                     nom = True              ham : spam   =    117.8 : 1.0\n",
      "                     ect = True              ham : spam   =    115.3 : 1.0\n",
      "                    2001 = True              ham : spam   =     71.7 : 1.0\n",
      "              nomination = True              ham : spam   =     68.8 : 1.0\n",
      "                     713 = True              ham : spam   =     64.9 : 1.0\n",
      "            prescription = True             spam : ham    =     50.8 : 1.0\n",
      "                     bob = True              ham : spam   =     48.0 : 1.0\n",
      "                  farmer = True              ham : spam   =     45.8 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcess With Removing Stop Words, Symbols, and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['http','%','$','_','#','|','&','*'\n",
    "                  ,',', '.', '-', '/', ':', '``', '`'\n",
    "                  , \"'\", \"...\", '--', '@', 'for','of'\n",
    "                  ,'?',')','(','>','=',';','!',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44399\n"
     ]
    }
   ],
   "source": [
    "stop_all_words_list = [word for (sent, cat) in emaildocs for word in sent if word not in stopwords]\n",
    "stop_all_words = nltk.FreqDist(stop_all_words_list)\n",
    "print(len(stop_all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1500 most frequently appearing keywords in the corpus\n",
    "stop_word_items = stop_all_words.most_common(4000)\n",
    "stop_word_features = [word for (word,count) in stop_word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features sets for a document, including keyword features and category feature\n",
    "stop_featuresets = [(stop_document_features(d, stop_word_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training using naive Baysian classifier, training set is roughly 80% of the data\n",
    "train_set, test_set = stop_featuresets[600:], stop_featuresets[:600]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9516666666666667"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the accuracy of the classifier\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 600\n",
      "0 0.9516666666666667\n",
      "1 0.9333333333333333\n",
      "2 0.9233333333333333\n",
      "3 0.9216666666666666\n",
      "4 0.9383333333333334\n",
      "mean accuracy 0.9336666666666668\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, stop_featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    \tgoldlist.append(label)\n",
    "    \tpredictedlist.append(classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam']\n",
      "['ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam']\n",
      "     |   s     |\n",
      "     |   p   h |\n",
      "     |   a   a |\n",
      "     |   m   m |\n",
      "-----+---------+\n",
      "spam |<296> 17 |\n",
      " ham |  12<275>|\n",
      "-----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the first 30 examples\n",
    "print(goldlist[:30])\n",
    "print(predictedlist[:30])\n",
    "\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output:  prints precision, recall and F1 for each label\n",
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.958      0.942      0.950\n",
      "spam \t      0.946      0.961      0.953\n"
     ]
    }
   ],
   "source": [
    "# call the function with our data\n",
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 1500 words = 94.5 accuracy; trying 4000 = 95.1; 2000 = 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               forwarded = True              ham : spam   =    197.3 : 1.0\n",
      "                     hou = True              ham : spam   =    186.3 : 1.0\n",
      "                     nom = True              ham : spam   =    117.8 : 1.0\n",
      "                     ect = True              ham : spam   =    115.3 : 1.0\n",
      "                    2001 = True              ham : spam   =     71.7 : 1.0\n",
      "              nomination = True              ham : spam   =     68.8 : 1.0\n",
      "                     713 = True              ham : spam   =     64.9 : 1.0\n",
      "            prescription = True             spam : ham    =     50.8 : 1.0\n",
      "                     bob = True              ham : spam   =     48.0 : 1.0\n",
      "                  farmer = True              ham : spam   =     45.8 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams with Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for using bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject', ':', 'accrued', 'thru', 'nov', '.', '19', 'daren', ',', 'please', 'take', 'a', 'look', 'at', 'the', 'attached', 'spreadsheet', ',', 'we', 'have', 'over', '500', ',', '000', 'mmbtu', 'as', 'unaccounted', 'for', '.', '.', '.', 'o', \"'\", 'neal', '3', '-', '9686', 'Subject', ':', 'dewbre', 'petroleum', 'vance', ',', 'the', 'following', 'deal', 'is', 'not', 'on', 'you']\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder on all the words in sequence\n",
    "print(all_words_list[:50])\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\x11', '63884831'), ('0184', 'bowy'), ('04005', 'jnkex'), ('04607001', 'easttrans'), ('0920', '8774937918'), ('0986315', 'shawnee'), ('0986511', 'huff'), ('1010', 'liverpool'), ('12278', 'hnbgd'), ('126360', '6742'), ('133168', 'gsf'), ('1361', 'comiats'), ('139067', 'danex'), ('1399', 'matagorda'), ('1447', 'bingolineprocessing'), ('17611', 'loring'), ('1767', 'clemmons'), ('17832', 'andl'), ('19436', 'auburndale'), ('19707', 'rackingham'), ('2073256', 'aebdb'), ('2086', 'slkhta'), ('2286', 'pxlsy'), ('238', '3164'), ('2392', '222215'), ('24679', 'coa'), ('2900', 'wilcrest'), ('3066', 'implications'), ('327964', '9844'), ('334', 'ndwsmkrsvpjlfygao'), ('357461850', '760844164'), ('361', '689894'), ('378232683', '15867384'), ('384', 'gosj'), ('3881', 'uosda'), ('4005', 'belen'), ('40937', 'bedford'), ('4120', '93883'), ('4135576', 'eaa'), ('415086', 'hlu'), ('416501', 'bvlmhht'), ('4234', 'frvklspqwhwrcu'), ('44000', 'nantes'), ('4501', 'shetland'), ('452601', 'pringle'), ('45337672', 'qo'), ('48352', 'fnxhhrhioidagqi'), ('48451', 'bentall'), ('5053', '93728'), ('5085', 'extention')]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# define the top 500 bigrams using the chi squared measure\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 1000)\n",
    "print(bigram_features[:50])\n",
    "print(len(bigram_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features that include words as before \n",
    "# add the most frequent significant bigrams\n",
    "# this function takes the list of words in a document as an argument and returns a feature dictionary\n",
    "# it depends on the variables word_features and bigram_features\n",
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to create feature sets for all sentences\n",
    "bigram_featuresets = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a classifier and report accuracy\n",
    "train_set, test_set = bigram_featuresets[1000:], bigram_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 600\n",
      "0 0.9816666666666667\n",
      "1 0.9533333333333334\n",
      "2 0.9533333333333334\n",
      "3 0.9366666666666666\n",
      "4 0.9716666666666667\n",
      "mean accuracy 0.9593333333333334\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, bigram_featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output:  prints precision, recall and F1 for each label\n",
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    \tgoldlist.append(label)\n",
    "    \tpredictedlist.append(classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam']\n",
      "['ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam']\n",
      "     |   s     |\n",
      "     |   p   h |\n",
      "     |   a   a |\n",
      "     |   m   m |\n",
      "-----+---------+\n",
      "spam |<493>  8 |\n",
      " ham |  22<477>|\n",
      "-----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the first 30 examples\n",
    "print(goldlist[:30])\n",
    "print(predictedlist[:30])\n",
    "\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.956      0.984      0.970\n",
      "spam \t      0.984      0.957      0.970\n"
     ]
    }
   ],
   "source": [
    "# call the function with our data\n",
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bclassifier.show_most_informative_features(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_document_features(document, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to create feature sets for all sentences\n",
    "bigram_featuresets = [(bigram_document_features(d, bigram_features), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.499"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a classifier and report accuracy\n",
    "train_set, test_set = bigram_featuresets[1000:], bigram_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 1034\n",
      "0 0.7088974854932302\n",
      "1 0.688588007736944\n",
      "2 0.730174081237911\n",
      "3 0.718568665377176\n",
      "4 0.7040618955512572\n",
      "mean accuracy 0.7100580270793037\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, bigram_featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    \tgoldlist.append(label)\n",
    "    \tpredictedlist.append(classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |       s |\n",
      "     |   h   p |\n",
      "     |   a   a |\n",
      "     |   m   m |\n",
      "-----+---------+\n",
      " ham |<708>  . |\n",
      "spam | 292  <.>|\n",
      "-----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-620-75ff4587f099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# call the function with our data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meval_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoldlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictedlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-576-90457b9fb95c>\u001b[0m in \u001b[0;36meval_measures\u001b[0;34m(gold, predicted)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mTN\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# use these to compute recall, precision, F1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrecall_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# call the function with our data\n",
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            B_\u0011_63884831 = False             ham : spam   =      1.0 : 1.0\n",
      "         B_118532_101473 = False            spam : ham    =      1.0 : 1.0\n",
      "    B_bwbllfxg_paprcfwzm = False            spam : ham    =      1.0 : 1.0\n",
      "       B_amoy_shrewsbury = False            spam : ham    =      1.0 : 1.0\n",
      "          B_cdexeg_fcenz = False            spam : ham    =      1.0 : 1.0\n",
      "B_boron_clemsonprimordial = False            spam : ham    =      1.0 : 1.0\n",
      "          B_ckpiw_eqgslv = False            spam : ham    =      1.0 : 1.0\n",
      "             B_2578_5767 = False            spam : ham    =      1.0 : 1.0\n",
      "          B_985853_riser = False            spam : ham    =      1.0 : 1.0\n",
      "          B_bluebush_nod = False            spam : ham    =      1.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of mixed spam and ham email documents as (list of words, label)\n",
    "POSemaildocs = []\n",
    "  # add all the spam\n",
    "for spam in spamtexts:\n",
    "    tokens = nltk.tokenize.sent_tokenize(spam)\n",
    "    POSemaildocs.append((tokens, 'spam'))\n",
    "  # add all the regular emails\n",
    "for ham in hamtexts:\n",
    "    tokens = nltk.tokenize.sent_tokenize(ham)\n",
    "    POSemaildocs.append((tokens, 'ham'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the list\n",
    "random.Random(123).shuffle(POSemaildocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Subject: accrued thru nov .', '19\\ndaren ,\\nplease take a look at the attached spreadsheet , we have over 500 , 000 mmbtu as\\nunaccounted for .', '.', '.', \"o ' neal 3 - 9686\"], 'ham')\n"
     ]
    }
   ],
   "source": [
    "# print a few token lists\n",
    "for email in POSemaildocs[:1]:\n",
    "    print (email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27740\n"
     ]
    }
   ],
   "source": [
    "stop_all_words_list = [word for (sent, cat) in POSemaildocs for word in sent if word not in stopwords]\n",
    "stop_all_words = nltk.FreqDist(stop_all_words_list)\n",
    "print(len(stop_all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44574\n"
     ]
    }
   ],
   "source": [
    "# get all words from all emails and put into a frequency distribution\n",
    "# note lowercase, but no stemming or stopwords\n",
    "pall_words_list = [word for (sent,cat) in POSemaildocs for word in sent]\n",
    "pall_words = nltk.FreqDist(all_words_list)\n",
    "print(len(pall_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1500 most frequently appearing keywords in the corpus\n",
    "word_items = stop_all_words.most_common(2000)\n",
    "word_features = [word for (word,count) in word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1500 most frequently appearing keywords in the corpus\n",
    "word_items = pall_words.most_common(4000)\n",
    "word_features = [word for (word,count) in word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a document list of words and returns a feature dictionary\n",
    "# it runs the default pos tagger (the Stanford tagger) on the document\n",
    "#   and counts 4 types of pos tags to use as features\n",
    "def POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27744\n"
     ]
    }
   ],
   "source": [
    "# define feature sets using this function\n",
    "POS_featuresets = [(POS_features(d, stop_all_words_list), c) for (d, c) in POSemaildocs]\n",
    "# number of features for document 0\n",
    "print(len(POS_featuresets[0][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Subject: dewbre petroleum\\nvance ,\\nthe following deal is not on you spreadsheet nor is it on the prebid list that daren maintains :\\ndeal # counterparty meter #\\n137595 dewbre petr .', '9662\\nis this a good deal ?', 'bob'], 'ham')\n",
      "num nouns 2\n",
      "num verbs 0\n",
      "num adjectives 0\n",
      "num adverbs 0\n"
     ]
    }
   ],
   "source": [
    "# the first sentence\n",
    "print(POSemaildocs[1])\n",
    "# the pos tag features for this sentence\n",
    "print('num nouns', POS_featuresets[1][0]['nouns'])\n",
    "print('num verbs', POS_featuresets[1][0]['verbs'])\n",
    "print('num adjectives', POS_featuresets[1][0]['adjectives'])\n",
    "print('num adverbs', POS_featuresets[1][0]['adverbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6366666666666667"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test the classifier\n",
    "ptrain_set, ptest_set = POS_featuresets[600:], POS_featuresets[:600]\n",
    "pclassifier = nltk.NaiveBayesClassifier.train(ptrain_set)\n",
    "nltk.classify.accuracy(pclassifier, ptest_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "      contains(thanks .) = True              ham : spam   =     63.0 : 1.0\n",
      "         contains(www .) = True             spam : ham    =     27.6 : 1.0\n",
      "         contains(net .) = True             spam : ham    =     27.6 : 1.0\n",
      "           contains(r .) = True             spam : ham    =     20.6 : 1.0\n",
      "contains(http : / / www .) = True             spam : ham    =     16.2 : 1.0\n",
      "          contains(html) = True             spam : ham    =     11.9 : 1.0\n",
      "           contains(j .) = True              ham : spam   =     11.4 : 1.0\n",
      "   contains(thank you !) = True              ham : spam   =      8.8 : 1.0\n",
      "           contains(m .) = True              ham : spam   =      7.2 : 1.0\n",
      "         contains(com /) = True             spam : ham    =      7.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pclassifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentence_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSubjectivity(path):\n",
    "    flexicon = open(path, 'r')\n",
    "    # initialize an empty dictionary\n",
    "    sldict = { }\n",
    "    for line in flexicon:\n",
    "        fields = line.split()   # default is to split on whitespace\n",
    "        # split each field on the '=' and keep the second part as the value\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        # put a dictionary entry with the word as the keyword\n",
    "        #     and a list of the other values\n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLpath = \"/Users/wa3/Syracuse/Term6/NLP/Week8/subjclueslen1-HLTEMNLP05.tff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL = readSubjectivity(SLpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6885"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SL.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SL_features(document, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "   \n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_featuresets = [(SL_features(d, SL), c) for (d, c) in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# show just the two sentiment lexicon features in document 0\n",
    "print(SL_featuresets[0][0]['positivecount'])\n",
    "print(SL_featuresets[0][0]['negativecount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this gives the label of document 0\n",
    "print(SL_featuresets[0][1])\n",
    "# number of features for document 0\n",
    "len(SL_featuresets[0][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6616666666666666"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrain the classifier using these features\n",
    "sltrain_set, sltest_set = SL_featuresets[600:], SL_featuresets[:600]\n",
    "slclassifier = nltk.NaiveBayesClassifier.train(sltrain_set)\n",
    "nltk.classify.accuracy(slclassifier, sltest_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "2400\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(SL_featuresets))\n",
    "print(len(sltrain_set))\n",
    "print(len(sltest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           negativecount = 16               spam : ham    =      8.5 : 1.0\n",
      "           negativecount = 15               spam : ham    =      6.5 : 1.0\n",
      "           positivecount = 30               spam : ham    =      6.5 : 1.0\n",
      "           negativecount = 8                spam : ham    =      6.4 : 1.0\n",
      "           negativecount = 10               spam : ham    =      5.4 : 1.0\n",
      "           negativecount = 12               spam : ham    =      4.7 : 1.0\n",
      "           negativecount = 7                spam : ham    =      4.4 : 1.0\n",
      "           positivecount = 27               spam : ham    =      4.4 : 1.0\n",
      "           negativecount = 18               spam : ham    =      3.9 : 1.0\n",
      "           positivecount = 37               spam : ham    =      3.7 : 1.0\n",
      "           positivecount = 24               spam : ham    =      3.6 : 1.0\n",
      "           positivecount = 28               spam : ham    =      3.5 : 1.0\n",
      "           negativecount = 5                spam : ham    =      3.3 : 1.0\n",
      "           negativecount = 6                spam : ham    =      3.2 : 1.0\n",
      "           negativecount = 9                spam : ham    =      3.1 : 1.0\n",
      "           positivecount = 29               spam : ham    =      3.1 : 1.0\n",
      "           positivecount = 19               spam : ham    =      2.8 : 1.0\n",
      "           positivecount = 14               spam : ham    =      2.5 : 1.0\n",
      "           positivecount = 23               spam : ham    =      2.5 : 1.0\n",
      "           positivecount = 26               spam : ham    =      2.5 : 1.0\n",
      "           positivecount = None              ham : spam   =      2.5 : 1.0\n",
      "           negativecount = None              ham : spam   =      2.5 : 1.0\n",
      "           negativecount = 27               spam : ham    =      2.4 : 1.0\n",
      "           positivecount = 16               spam : ham    =      2.3 : 1.0\n",
      "           positivecount = 15               spam : ham    =      2.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(slclassifier.show_most_informative_features(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT_features(document, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the feature sets\n",
    "NOT_featuresets = [(NOT_features(d, negationwords), c) for (d, c) in emaildocs]\n",
    "# show the values of a couple of example features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             V_forwarded = True              ham : spam   =    165.3 : 1.0\n",
      "                   V_hou = True              ham : spam   =    156.7 : 1.0\n",
      "                   V_ect = True              ham : spam   =     97.2 : 1.0\n",
      "                   V_nom = True              ham : spam   =     96.8 : 1.0\n",
      "                  V_2001 = True              ham : spam   =     59.7 : 1.0\n",
      "                   V_713 = True              ham : spam   =     58.2 : 1.0\n",
      "                    V_cc = True              ham : spam   =     39.9 : 1.0\n",
      "                V_farmer = True              ham : spam   =     39.8 : 1.0\n",
      "                   V_bob = True              ham : spam   =     39.7 : 1.0\n",
      "          V_prescription = True             spam : ham    =     38.4 : 1.0\n",
      "                     V_| = True             spam : ham    =     37.7 : 1.0\n",
      "            V_compliance = True             spam : ham    =     35.1 : 1.0\n",
      "                 V_vance = True              ham : spam   =     34.3 : 1.0\n",
      "         V_international = True             spam : ham    =     33.7 : 1.0\n",
      "                  V_lisa = True              ham : spam   =     32.3 : 1.0\n",
      "                 V_susan = True              ham : spam   =     30.9 : 1.0\n",
      "                  V_duke = True              ham : spam   =     30.9 : 1.0\n",
      "              V_attached = True              ham : spam   =     30.9 : 1.0\n",
      "               V_revised = True              ham : spam   =     29.6 : 1.0\n",
      "                 V_steve = True              ham : spam   =     28.3 : 1.0\n",
      "                V_meyers = True              ham : spam   =     27.6 : 1.0\n",
      "               V_NOTmore = True             spam : ham    =     27.1 : 1.0\n",
      "                 V_plain = False            spam : ham    =     27.1 : 1.0\n",
      "              V_property = True             spam : ham    =     27.1 : 1.0\n",
      "                V_meters = True              ham : spam   =     26.9 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b: SkLearn Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, testing_set = featuresets[600:], featuresets[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy percent: 97.83333333333334\n",
      "BernoulliNB_classifier accuracy percent: 98.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 97.66666666666667\n",
      "SGDClassifier_classifier accuracy percent: 97.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC_classifier accuracy percent: 91.83333333333333\n",
      "LinearSVC_classifier accuracy percent: 97.33333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy percent: 96.33333333333334\n",
      "DecisionTree_classifier accuracy percent: 95.16666666666667\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "DecisionTree_classifier = SklearnClassifier(DecisionTreeClassifier())\n",
    "DecisionTree_classifier.train(training_set)\n",
    "print(\"DecisionTree_classifier accuracy percent:\", (nltk.classify.accuracy(DecisionTree_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    \tgoldlist.append(label)\n",
    "    \tpredictedlist.append(LogisticRegression_classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    \tgoldlist.append(label)\n",
    "    \tpredictedlist.append(BernoulliNB_classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam']\n",
      "['ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham', 'spam']\n",
      "     |   s     |\n",
      "     |   p   h |\n",
      "     |   a   a |\n",
      "     |   m   m |\n",
      "-----+---------+\n",
      "spam |<313>  . |\n",
      " ham |  12<275>|\n",
      "-----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the first 30 examples\n",
    "print(goldlist[:30])\n",
    "print(predictedlist[:30])\n",
    "\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output:  prints precision, recall and F1 for each label\n",
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.958      1.000      0.979\n",
      "spam \t      1.000      0.963      0.981\n"
     ]
    }
   ],
   "source": [
    "# call the function with our data\n",
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
